{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "funky-frank",
   "metadata": {},
   "source": [
    "In this notebook, we will dive a little into unsupervised sentiment analysis using the SentiWordNet lexicon. We will use `nltk`, a very well known linrary for NLP.\n",
    "\n",
    "You will need the following libraries installed: `nltk`, `numpy`, and `dataclasses` (this one you can avoid by changing the code a bit). I ran this on Python 3.6 but any Python 3 should work. These are the versions on my computer: \n",
    "\n",
    "```\n",
    "In [65]: numpy.__version__\n",
    "Out[65]: '1.18.4'\n",
    "\n",
    "In [66]: nltk.__version__\n",
    "Out[66]: '3.5'\n",
    "```\n",
    "\n",
    "First, some necessary imports and downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "super-chuck",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n[nltk_data]     C:\\Users\\Bence\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package sentiwordnet is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\Bence\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Usage:   \n",
      "  C:\\ProgramData\\Miniconda3\\python.exe -m pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  C:\\ProgramData\\Miniconda3\\python.exe -m pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  C:\\ProgramData\\Miniconda3\\python.exe -m pip install [options] [-e] <vcs project url> ...\n",
      "  C:\\ProgramData\\Miniconda3\\python.exe -m pip install [options] [-e] <local project path> ...\n",
      "  C:\\ProgramData\\Miniconda3\\python.exe -m pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -–\n"
     ]
    }
   ],
   "source": [
    "pip install -–upgrade pip."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "accepting-performance",
   "metadata": {},
   "source": [
    "If you remember from the slides, SentiWordNet is a big dictionary (map) where each [synset](https://www.geeksforgeeks.org/nlp-synsets-for-a-word-in-wordnet/) is indexed by a key (Part of speech tag, Word). It gives you the positive score, negative score and neutral scores for that key (along with the synset). \n",
    "\n",
    "What is a part of speech tagging? The process of classifying words into their parts of speech and labeling them accordingly.\n",
    "\n",
    "| sentence                                                 | why    | not    | tell | someone | ?           |\n",
    "|----------------------------------------------------------|--------|--------|------|---------|-------------|\n",
    "| part of speech                                           | adverb | adverb | verb | noun    | punctuation |\n",
    "| [part of speech tag](http://www.nltk.org/book/ch05.html) | WRB    | RB     | VB   | NN      | .           |\n",
    "\n",
    "Any part of speech tagger will use a particular `tagset`: or a set of part of speech tags. NLTK uses a tagset from [UPenn](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html), more commonly known as PennTreeBank. \n",
    "\n",
    "How to get sentiment from SentiWordNet?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acoustic-disability",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[SentiSynset('happy.a.01'), SentiSynset('felicitous.s.02'), SentiSynset('glad.s.02'), SentiSynset('happy.s.04')]\n------------------------------\nword: happy, positive score: 0.875\nword: happy, negative score: 0.0\nword: happy, neutral score: 0.125\n------------------------------\n------------------------------\nword: unhappy, positive score: 0.0\nword: unhappy, negative score: 0.75\nword: unhappy, neutral score: 0.25\n------------------------------\n"
     ]
    }
   ],
   "source": [
    "positive_word = \"happy\"\n",
    "negative_word = \"unhappy\"\n",
    "pos_tag = \"a\"\n",
    "print(list(swn.senti_synsets(positive_word, pos_tag)))\n",
    "for word in [positive_word, negative_word]:\n",
    "    print(\"-\"*30)\n",
    "    print(f\"word: {word}, positive score: {list(swn.senti_synsets(word, pos_tag))[0].pos_score()}\")\n",
    "    print(f\"word: {word}, negative score: {list(swn.senti_synsets(word, pos_tag))[0].neg_score()}\")\n",
    "    print(f\"word: {word}, neutral score: {list(swn.senti_synsets(word, pos_tag))[0].obj_score()}\")\n",
    "    print(\"-\"*30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-companion",
   "metadata": {},
   "source": [
    "You might have noticed that we use a PoS tag of `a` and not `ADJ`. This is because SentiWordNet uses a different tagset for part of speech tags: `a: all adjectives, r: all adverbs, v: all verbs, n: all nouns`. This requires us to define this function later:\n",
    "\n",
    "```\n",
    "def penn_pos_tag_to_word_net(pos_tag_penn: str) -> Union[str, None]:\n",
    "    word_net_tag = {'NN':wn.NOUN, 'JJ':wn.ADJ,\n",
    "                  'VB':wn.VERB, 'RB':wn.ADV}\n",
    "    return word_net_tag.get(pos_tag_penn[:2])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-playlist",
   "metadata": {},
   "source": [
    "So we know how to get positive, negative, and neutral sentiment score for a word from SentiWordNet. How do we use that to classify a sentence? Let's define some interfaces first.\n",
    "\n",
    "We will assume the use of SentiWordNet, so some parts of the code are the same for any class, i.e, tokenization, pos tagging, conversion of pos tags to SentiWordNet pos tags and scores for each sentiment. The change is in _how_ we use these scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hidden-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Literal\n",
    "from nltk.tokenize import word_tokenize as tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "from typing import List, Tuple, Union\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class Sentiment(Enum):\n",
    "    POSITIVE = 1\n",
    "    NEGATIVE = 2\n",
    "    NEUTRAL = 3\n",
    "\n",
    "@dataclass\n",
    "class TokenSentiment:\n",
    "    token: str\n",
    "    pos_tag: str \n",
    "    positive: str\n",
    "    negative: str \n",
    "    neutral: str\n",
    "\n",
    "def penn_pos_tag_to_word_net(pos_tag_penn: str) -> Union[str, None]:\n",
    "    word_net_tag = {'NN':wn.NOUN, 'JJ':wn.ADJ,\n",
    "                  'VB':wn.VERB, 'RB':wn.ADV}\n",
    "    print(pos_tag_penn)\n",
    "    return word_net_tag.get(pos_tag_penn[:2])\n",
    "\n",
    "\n",
    "def get_token_sentiment(token: str, pos: str) -> TokenSentiment:\n",
    "    try:\n",
    "        synset_0 = list(swn.senti_synsets(token, pos))[0]\n",
    "        return TokenSentiment(token=token, pos_tag=pos, positive=synset_0.pos_score(), negative=synset_0.neg_score(), \n",
    "                          neutral=synset_0.obj_score())\n",
    "    except IndexError:\n",
    "        return TokenSentiment(token=token, pos_tag=pos, positive=0, negative=0,  neutral=1.) \n",
    "\n",
    "class SentenceSentiment:\n",
    "    def __init__(self, sentence: str):\n",
    "        self.sentence = sentence\n",
    "        self.pos_tokens = [(token, penn_pos_tag_to_word_net(pos_tag)) for token, pos_tag in pos_tag(tokenize(sentence))]\n",
    "        self.token_sentiments = [get_token_sentiment(token, pos) for token, pos in self.pos_tokens if pos is not None]     \n",
    "    \n",
    "    def run(self, **kwargs) -> Literal[Sentiment.POSITIVE, Sentiment.NEGATIVE, Sentiment.NEUTRAL]:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "reasonable-chicken",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Bence/nltk_data'\n    - 'C:\\\\ProgramData\\\\Miniconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Miniconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Miniconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Bence\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f7685ab60bec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# sentence = SentenceSentiment(\"this movie is awesome!\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentenceSentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"This movie is awesome!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_sentiments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-6cd2d45c8cfd>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpenn_pos_tag_to_word_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_tag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_sentiments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_token_sentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m     return [\n\u001b[0;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nltk\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 877\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    878\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Bence/nltk_data'\n    - 'C:\\\\ProgramData\\\\Miniconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Miniconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Miniconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Bence\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# sentence = SentenceSentiment(\"this movie is awesome!\")\n",
    "sentence = SentenceSentiment(\"This movie is awesome!\")\n",
    "sentence.token_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-athletics",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = SentenceSentiment(\"this movie is horrible!\")\n",
    "sentence.token_sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-sailing",
   "metadata": {},
   "source": [
    "Now we will implement the `run` method in a subclass to get the sentiment of a sentence. A simple way to do this would be provide an aggregation of the sentiment scores of the tokens. The aggregation method can be average or max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-possible",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SentenceSentimentAggregation(SentenceSentiment):\n",
    "    def __init__(self, sentence: str):\n",
    "        super().__init__(sentence)\n",
    "    \n",
    "    def run(self, **kwargs):\n",
    "        aggregation_fn = kwargs['aggregation_fn']\n",
    "        score_dict = {\n",
    "            Sentiment.POSITIVE: aggregation_fn([x.positive for x in self.token_sentiments]),\n",
    "            Sentiment.NEGATIVE: aggregation_fn([x.negative for x in self.token_sentiments]),\n",
    "            Sentiment.NEUTRAL: aggregation_fn([x.neutral for x in self.token_sentiments]),\n",
    "        }\n",
    "        score_vals = score_dict.items()\n",
    "        return sorted(score_vals, key=lambda x:x[1], reverse=True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = SentenceSentimentAggregation(\"this movie is awesome!\")\n",
    "sentence.run(aggregation_fn=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = SentenceSentimentAggregation(\"awesome!\")\n",
    "sentence.run(aggregation_fn=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = SentenceSentimentAggregation(\"truly awesome!\")\n",
    "sentence.run(aggregation_fn=np.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-diagnosis",
   "metadata": {},
   "source": [
    "You should try this out with some other examples. In another method, you can take the number of positive vs number of negative words to determine the polarity of the sentence. This is left for exercise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de",
   "display_name": "Python 3.8.5 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}