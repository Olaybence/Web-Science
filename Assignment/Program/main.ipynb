{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de",
   "display_name": "Python 3.8.5 64-bit (conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Stanford Data-set (SST) - Rotten Tomatoes reviews\n",
    "## Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class SentAnalyzer():\n",
    "    def __init__(self, sourcePath):\n",
    "        self.data = pd.read_csv(sourcePath)\n",
    "        \n",
    "        # self.indexes = self.data['index']\n",
    "        self.labels = self.data['label']\n",
    "        \n",
    "        # Do we wanna clean the data?\n",
    "        self.cleanData = True\n",
    "\n",
    "        # self.texts = self.data['text']\n",
    "        self.texts = self.data[\"text\"]\n",
    "        self.textsL1 = self.data.loc[self.data['label'] == 1][\"text\"]\n",
    "        self.textsL0 = self.data.loc[self.data['label'] == 0][\"text\"]\n",
    "\n",
    "        self.stop_words = stopwords.words('english')\n",
    "        self.regexStopWords = \"\"\n",
    "        self.ws = \"( |\\W)\"\n",
    "        for w in self.stop_words:\n",
    "            self.regexStopWords += self.ws + w + self.ws + \"|\"\n",
    "            # self.regexStopWords += w + self.ws + \"|\"\n",
    "        self.regexStopWords = self.regexStopWords[:len(self.regexStopWords)-1]\n",
    "\n",
    "    def cleanString(self,text):\n",
    "        x = \" \" + text.lower()\n",
    "        x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
    "        x = re.sub('(%s)' % re.escape(self.regexStopWords), ' ', x)\n",
    "        print(x)\n",
    "        return x\n",
    "\n",
    "    def checkBalance(self):\n",
    "        len1 = len(self.textsL1)\n",
    "        len0 = len(self.textsL0)\n",
    "        print('Label 1:',len1)\n",
    "        print('Label 0:',len0)\n",
    "        if len1 > len0:\n",
    "            print('Their ratio:',round(abs(len1/len0),2))\n",
    "        else:\n",
    "            print('Their ratio:',round(abs(len0/len1),2))\n",
    "\n",
    "    def nGrams(self, data, minN,maxN,n):\n",
    "        for i in range(minN,maxN):\n",
    "            cv = CountVectorizer(ngram_range = (i,i))\n",
    "            if self.cleanData:\n",
    "                cv = CountVectorizer(ngram_range = (i,i),preprocessor=self.cleanString)\n",
    "\n",
    "            word_count_vector = cv.fit_transform(data)\n",
    "            # print(word_count_vector)\n",
    "            \n",
    "            # Fit the model into the data\n",
    "            tfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "            tfidf_transformer.fit(word_count_vector)\n",
    "            \n",
    "            df_idf = pd.DataFrame(tfidf_transformer.idf_, index = cv.get_feature_names(),columns = [\"tfidf\"])\n",
    "            \n",
    "            # sort ascending \n",
    "            print(df_idf.sort_values(by = ['tfidf'])[:n])\n",
    "    \n",
    "    def lengthCorrelation(self):\n",
    "        lengths = [(lambda x: len(x[1]))(x) for x in self.texts.items()]\n",
    "        lenLab = pd.DataFrame( list(zip(lengths, self.labels)), columns = [\"length\",\"labels\"])\n",
    "        print(lenLab.corr())\n",
    "\n",
    "    def textTfidfValues(self,data):\n",
    "        # this steps generates word counts for the words in your docs \n",
    "        cv = CountVectorizer()\n",
    "        if self.cleanData:\n",
    "            cv = CountVectorizer(preprocessor = self.cleanString)\n",
    "        \n",
    "        word_count_vector = cv.fit_transform(data)\n",
    "\n",
    "        # print(word_count_vector)\n",
    "        \n",
    "        tfidf_transformer = TfidfTransformer(smooth_idf = True, use_idf = True)\n",
    "        tfidf_transformer.fit(word_count_vector)\n",
    "        \n",
    "        self.tfidf = pd.DataFrame(tfidf_transformer.idf_, index = cv.get_feature_names(), columns = [\"tfidf\"])\n",
    "        \n",
    "        # sort ascending \n",
    "        # print(self.tfidf_transformer.idf_)\n",
    "        return self.tfidf.sort_values(by = [\"tfidf\"])"
   ]
  },
  {
   "source": [
    "# Load the SST data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "train = SentAnalyzer(\"stsa.binary.phrases.train\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 94,
   "outputs": []
  },
  {
   "source": [
    "## Testing cleaning data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "From:\n the worst revenge-of-the-nerds clichés the filmmakers could dredge up\nTo:\n the worst revenge-of-the-nerds clichés the filmmakers could dredge up\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' the worst revenge of the nerds clichés the filmmakers could dredge up '"
      ]
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "stop_words = [\"the\"]\n",
    "regexStopWords = \"\"\n",
    "ws = \"( )\"\n",
    "for w in stop_words:\n",
    "    regexStopWords += ws + w + ws + \"|\"\n",
    "    # regexStopWords += w + ws + \"|\"\n",
    "regexStopWords = regexStopWords[:len(regexStopWords)-1]\n",
    "def cleanString(text):\n",
    "    print(\"From:\\n\",text)\n",
    "    x = \" \" + text.lower() + \" \"\n",
    "    x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
    "    x = re.sub('(%s)' % re.escape(regexStopWords), ' ', x)\n",
    "    print(\"To:\\n\",text)\n",
    "    return x\n",
    "\n",
    "text = \"the worst revenge-of-the-nerds clichés the filmmakers could dredge up\"\n",
    "cleanString(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.data[[\"label\",\"text\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The 20 most common word overall and within the two label.\n",
      "Overall:\n",
      "          tfidf\n",
      "the    2.210299\n",
      "and    2.367068\n",
      "of     2.490469\n",
      "to     2.807791\n",
      "is     3.125087\n",
      "that   3.243210\n",
      "in     3.278603\n",
      "it     3.374383\n",
      "with   3.699859\n",
      "film   3.829284\n",
      "an     3.847267\n",
      "for    3.871987\n",
      "as     3.890294\n",
      "its    3.916836\n",
      "movie  3.927944\n",
      "this   4.036873\n",
      "but    4.166196\n",
      "be     4.231125\n",
      "on     4.323021\n",
      "you    4.358615\n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "print(\"The\", n ,\"most common word overall and within the two label.\")\n",
    "print(\"Overall:\")\n",
    "print(train.textTfidfValues( train.texts.values ).iloc[:n])\n",
    "# print(\"Label 0:\")\n",
    "# print(train.textTfidfValues(train.textsL0).iloc[:n])\n",
    "# print(\"Label 1:\")\n",
    "# print(train.textTfidfValues(train.textsL1).iloc[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Whole database\n"
     ]
    }
   ],
   "source": [
    "# Get the n-grams\n",
    "print(\"Whole database\")\n",
    "# train.nGrams(train.texts, 1, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Label 1 sentences\n"
     ]
    }
   ],
   "source": [
    "# Get the n-grams of Label 1\n",
    "print(\"Label 1 sentences\")\n",
    "# train.nGrams(train.textsL1, 1, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Label 0 sentences\n"
     ]
    }
   ],
   "source": [
    "# Get the n-grams of Label 0\n",
    "print(\"Label 0 sentences\")\n",
    "# train.nGrams(train.textsL0, 1, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "          length    labels\nlength  1.000000 -0.037769\nlabels -0.037769  1.000000\n"
     ]
    }
   ],
   "source": [
    "train.lengthCorrelation()"
   ]
  },
  {
   "source": [
    "# Amazon Review data-set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from urllib.request import urlopen\n",
    "\n",
    "###################\n",
    "## Amazon Review ##\n",
    "###################\n",
    "\n",
    "# Load in the Amazon Review Data with 5-core\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('Software_5.json.gz')\n",
    "df = df.fillna('')\n",
    "# print('shape', df.shape)\n",
    "\n",
    "products = df[\"asin\"].drop_duplicates()\n",
    "reviewers = df[\"reviewerID\"].drop_duplicates()\n",
    "# print('Products\\n',products)\n",
    "# print('Reviewers\\n',reviewers)\n",
    "\n",
    "# df.loc[df['reviewerID'] == ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    overall  verified   reviewTime      reviewerID        asin  \\\n",
       "0       4.0     False  10 20, 2010  A38NELQT98S4H8  0321719816   \n",
       "1       4.0     False  10 18, 2010  A3QJU4FEN8PQSZ  0321719816   \n",
       "2       5.0     False  10 16, 2010   ACJT8MUC0LRF0  0321719816   \n",
       "3       5.0     False  10 12, 2010   AYUF7YETYOLNX  0321719816   \n",
       "4       5.0     False   10 7, 2010  A31ICLWQ9CSHRS  0321719816   \n",
       "5       5.0     False  09 26, 2010  A2BVNVJOFXGZUB  0321719816   \n",
       "6       5.0     False   04 7, 2011  A2JMJVNTBL7K7E  0321719816   \n",
       "7       5.0     False   01 8, 2011  A14JBDSWKPKTZA  0321719816   \n",
       "8       5.0     False  10 28, 2010  A2WCFDOCS73PNB  0321719816   \n",
       "9       5.0     False  10 28, 2010  A14638TGYH7GD9  0321719816   \n",
       "10      3.0     False  10 21, 2010  A2VWE5SGNDS8HW  0321719816   \n",
       "11      5.0     False  10 13, 2010  A13700AF4X40YG  0321700945   \n",
       "12      5.0     False   10 7, 2010  A23E9QQHJLNGUI  0321700945   \n",
       "13      5.0     False  05 15, 2011  A1GQRGB8FGSLIZ  0321700945   \n",
       "14      3.0     False   01 9, 2011  A1E50L7PCVXLN4  0321700945   \n",
       "15      5.0     False   12 6, 2010  A10Y058K7B96C6  0321700945   \n",
       "16      5.0     False  11 15, 2010  A3V7D0LH8L7BG0  0321700945   \n",
       "17      4.0     False  11 27, 2010   AGVWTYW0ULXHT  0321719824   \n",
       "18      4.0     False  10 21, 2010  A3QJU4FEN8PQSZ  0321719824   \n",
       "19      4.0     False  10 10, 2010  A31N0XY2UTB25C  0321719824   \n",
       "\n",
       "                      style        reviewerName  \\\n",
       "0   {'Format:': ' DVD-ROM'}           WB Halper   \n",
       "1   {'Format:': ' DVD-ROM'}              Grimmy   \n",
       "2   {'Format:': ' DVD-ROM'}           D. Fowler   \n",
       "3   {'Format:': ' DVD-ROM'}        Bryan Newman   \n",
       "4   {'Format:': ' DVD-ROM'}          Al Swanson   \n",
       "5   {'Format:': ' DVD-ROM'}           J. Howard   \n",
       "6   {'Format:': ' DVD-ROM'}          Yesuaini99   \n",
       "7   {'Format:': ' DVD-ROM'}          Bob Feeser   \n",
       "8   {'Format:': ' DVD-ROM'}           Gadgetman   \n",
       "9   {'Format:': ' DVD-ROM'}       Rebecca Haden   \n",
       "10  {'Format:': ' DVD-ROM'}              tachi1   \n",
       "11  {'Format:': ' DVD-ROM'}   Loves Books in MD   \n",
       "12  {'Format:': ' DVD-ROM'}        Bill Oterson   \n",
       "13  {'Format:': ' DVD-ROM'}     Amazon Customer   \n",
       "14  {'Format:': ' DVD-ROM'}             Colinda   \n",
       "15  {'Format:': ' DVD-ROM'}         midnight122   \n",
       "16  {'Format:': ' DVD-ROM'}            Spike D.   \n",
       "17  {'Format:': ' DVD-ROM'}                Nate   \n",
       "18  {'Format:': ' DVD-ROM'}              Grimmy   \n",
       "19  {'Format:': ' DVD-ROM'}  Stephanie Sullivan   \n",
       "\n",
       "                                           reviewText  \\\n",
       "0   I've been using Dreamweaver (and it's predeces...   \n",
       "1   The demo is done with the PC version, with ref...   \n",
       "2   If you've been wanting to learn how to create ...   \n",
       "3   I've been creating websites with Dreamweaver f...   \n",
       "4   I decided (after trying a number of other prod...   \n",
       "5   The video is well-paced and delivered in an un...   \n",
       "6   I spent several hours on the lesson and I love...   \n",
       "7   I have had Dreamweaver MX2004 since it came ou...   \n",
       "8   I have also taken a local community college on...   \n",
       "9   Even though I use Dreamweaver a great deal and...   \n",
       "10  I waited to complete the entire course before ...   \n",
       "11  As someone who has just upgraded from Lightroo...   \n",
       "12  Adobe approved \"Adobe Photoshop Lightroom 3, L...   \n",
       "13  This is a greate collection of videos on the d...   \n",
       "14  There are over 100 video lessons here. Most us...   \n",
       "15  I am not an avid Lightroom user, although I ha...   \n",
       "16  I am a long time user of Photoshop Lightroom, ...   \n",
       "17  The \"Learn by Video\" program for Flash CS5 wou...   \n",
       "18  The presenter here sounds much more natural th...   \n",
       "19  This certified associate courseware comes as a...   \n",
       "\n",
       "                                              summary  unixReviewTime vote  \\\n",
       "0                 A solid overview of Dreamweaver CS5      1287532800        \n",
       "1                                        A good value      1287360000        \n",
       "2   This is excellent software for those who want ...      1287187200    3   \n",
       "3   A Fantastic Overview of Dream Weaver and Web D...      1286841600        \n",
       "4                                Excellent Tutorials!      1286409600        \n",
       "5                                          Excellent.      1285459200        \n",
       "6                   excellent video training material      1302134400        \n",
       "7        Great Video for a Difficult at Times Program      1294444800        \n",
       "8                       Excellent value for the price      1288224000    4   \n",
       "9              Buy this with your copy of Dreamweaver      1288224000    6   \n",
       "10  Competent introduction to Dreamweaver and web ...      1287619200   22   \n",
       "11  Learn Adobe Photoshop Lightroom 3 by Video (Le...      1286928000        \n",
       "12                               Absolutely the best.      1286409600   19   \n",
       "13            I find myself going back to the product      1305417600        \n",
       "14            For Highly Motivated and Patient People      1294531200   16   \n",
       "15  Great for beginners or just about any Lightroo...      1291593600        \n",
       "16                            You need to have this..      1289779200        \n",
       "17  A helpful introduction to Flash CS5 - a bit dr...      1290816000        \n",
       "18  A good introduction to Flash, with some UI wea...      1287619200        \n",
       "19        Learn Adobe Flash Professional CS5 by Video      1286668800        \n",
       "\n",
       "   image  \n",
       "0         \n",
       "1         \n",
       "2         \n",
       "3         \n",
       "4         \n",
       "5         \n",
       "6         \n",
       "7         \n",
       "8         \n",
       "9         \n",
       "10        \n",
       "11        \n",
       "12        \n",
       "13        \n",
       "14        \n",
       "15        \n",
       "16        \n",
       "17        \n",
       "18        \n",
       "19        "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>overall</th>\n      <th>verified</th>\n      <th>reviewTime</th>\n      <th>reviewerID</th>\n      <th>asin</th>\n      <th>style</th>\n      <th>reviewerName</th>\n      <th>reviewText</th>\n      <th>summary</th>\n      <th>unixReviewTime</th>\n      <th>vote</th>\n      <th>image</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.0</td>\n      <td>False</td>\n      <td>10 20, 2010</td>\n      <td>A38NELQT98S4H8</td>\n      <td>0321719816</td>\n      <td>{'Format:': ' DVD-ROM'}</td>\n      <td>WB Halper</td>\n      <td>I've been using Dreamweaver (and it's predeces...</td>\n      <td>A solid overview of Dreamweaver CS5</td>\n      <td>1287532800</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.0</td>\n      <td>False</td>\n      <td>10 18, 2010</td>\n      <td>A3QJU4FEN8PQSZ</td>\n      <td>0321719816</td>\n      <td>{'Format:': ' DVD-ROM'}</td>\n      <td>Grimmy</td>\n      <td>The demo is done with the PC version, with ref...</td>\n      <td>A good value</td>\n      <td>1287360000</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5.0</td>\n      <td>False</td>\n      <td>10 16, 2010</td>\n      <td>ACJT8MUC0LRF0</td>\n      <td>0321719816</td>\n      <td>{'Format:': ' DVD-ROM'}</td>\n      <td>D. Fowler</td>\n      <td>If you've been wanting to learn how to create ...</td>\n      <td>This is excellent software for those who want ...</td>\n      <td>1287187200</td>\n      <td>3</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.0</td>\n      <td>False</td>\n      <td>10 12, 2010</td>\n      <td>AYUF7YETYOLNX</td>\n      <td>0321719816</td>\n      <td>{'Format:': ' DVD-ROM'}</td>\n      <td>Bryan Newman</td>\n      <td>I've been creating websites with Dreamweaver f...</td>\n      <td>A Fantastic Overview of Dream Weaver and Web D...</td>\n      <td>1286841600</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>False</td>\n      <td>10 7, 2010</td>\n      <td>A31ICLWQ9CSHRS</td>\n      <td>0321719816</td>\n      <td>{'Format:': ' DVD-ROM'}</td>\n      <td>Al Swanson</td>\n      <td>I decided (after trying a number of other prod...</td>\n      <td>Excellent Tutorials!</td>\n      <td>1286409600</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5.0</td>\n      <td>False</td>\n      <td>09 26, 2010</td>\n      <td>A2BVNVJOFXGZUB</td>\n      <td>0321719816</td>\n      <td>{'Format:': ' DVD-ROM'}</td>\n      <td>J. Howard</td>\n      <td>The video is well-paced and delivered in an un...</td>\n      <td>Excellent.</td>\n      <td>1285459200</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>5.0</td>\n      <td>False</td>\n      <td>04 7, 2011</td>\n      <td>A2JMJVNTBL7K7E</td>\n      <td>0321719816</td>\n      <td>{'Format:': ' DVD-ROM'}</td>\n      <td>Yesuaini99</td>\n      <td>I spent several hours on the lesson and I love...</td>\n      <td>excellent video training material</td>\n      <td>1302134400</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>5.0</td>\n      <td>False</td>\n      <td>01 8, 2011</td>\n      <td>A14JBDSWKPKTZA</td>\n      <td>0321719816</td>\n      <td>{'Format:': ' DVD-ROM'}</td>\n      <td>Bob Feeser</td>\n      <td>I have had Dreamweaver MX2004 since it came ou...</td>\n      <td>Great Video for a Difficult at Times Program</td>\n      <td>1294444800</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>5.0</td>\n      <td>False</td>\n      <td>10 28, 2010</td>\n      <td>A2WCFDOCS73PNB</td>\n      <td>0321719816</td>\n      <td>{'Format:': ' DVD-ROM'}</td>\n      <td>Gadgetman</td>\n      <td>I have also taken a local community college on...</td>\n      <td>Excellent value for the price</td>\n      <td>1288224000</td>\n      <td>4</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>5.0</td>\n      <td>False</td>\n      <td>10 28, 2010</td>\n      <td>A14638TGYH7GD9</td>\n      <td>0321719816</td>\n      <td>{'Format:': ' DVD-ROM'}</td>\n      <td>Rebecca Haden</td>\n      <td>Even though I use Dreamweaver a great deal and...</td>\n      <td>Buy this with your copy of Dreamweaver</td>\n      <td>1288224000</td>\n      <td>6</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>3.0</td>\n      <td>False</td>\n      <td>10 21, 2010</td>\n      <td>A2VWE5SGNDS8HW</td>\n      <td>0321719816</td>\n      <td>{'Format:': ' DVD-ROM'}</td>\n      <td>tachi1</td>\n      <td>I waited to complete the entire course before ...</td>\n      <td>Competent introduction to Dreamweaver and web ...</td>\n      <td>1287619200</td>\n      <td>22</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>5.0</td>\n      <td>False</td>\n      <td>10 13, 2010</td>\n      <td>A13700AF4X40YG</td>\n      <td>0321700945</td>\n      <td>{'Format:': ' DVD-ROM'}</td>\n      <td>Loves Books in MD</td>\n      <td>As someone who has just upgraded from Lightroo...</td>\n      <td>Learn Adobe Photoshop Lightroom 3 by Video (Le...</td>\n      <td>1286928000</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>5.0</td>\n      <td>False</td>\n      <td>10 7, 2010</td>\n      <td>A23E9QQHJLNGUI</td>\n      <td>0321700945</td>\n      <td>{'Format:': ' DVD-ROM'}</td>\n      <td>Bill Oterson</td>\n      <td>Adobe approved \"Adobe Photoshop Lightroom 3, L...</td>\n      <td>Absolutely the best.</td>\n      <td>1286409600</td>\n      <td>19</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>5.0</td>\n      <td>False</td>\n      <td>05 15, 2011</td>\n      <td>A1GQRGB8FGSLIZ</td>\n      <td>0321700945</td>\n      <td>{'Format:': ' DVD-ROM'}</td>\n      <td>Amazon Customer</td>\n      <td>This is a greate collection of videos on the d...</td>\n      <td>I find myself going back to the product</td>\n      <td>1305417600</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>3.0</td>\n      <td>False</td>\n      <td>01 9, 2011</td>\n      <td>A1E50L7PCVXLN4</td>\n      <td>0321700945</td>\n      <td>{'Format:': ' DVD-ROM'}</td>\n      <td>Colinda</td>\n      <td>There are over 100 video lessons here. Most us...</td>\n      <td>For Highly Motivated and Patient People</td>\n      <td>1294531200</td>\n      <td>16</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>5.0</td>\n      <td>False</td>\n      <td>12 6, 2010</td>\n      <td>A10Y058K7B96C6</td>\n      <td>0321700945</td>\n      <td>{'Format:': ' DVD-ROM'}</td>\n      <td>midnight122</td>\n      <td>I am not an avid Lightroom user, although I ha...</td>\n      <td>Great for beginners or just about any Lightroo...</td>\n      <td>1291593600</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>5.0</td>\n      <td>False</td>\n      <td>11 15, 2010</td>\n      <td>A3V7D0LH8L7BG0</td>\n      <td>0321700945</td>\n      <td>{'Format:': ' DVD-ROM'}</td>\n      <td>Spike D.</td>\n      <td>I am a long time user of Photoshop Lightroom, ...</td>\n      <td>You need to have this..</td>\n      <td>1289779200</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>4.0</td>\n      <td>False</td>\n      <td>11 27, 2010</td>\n      <td>AGVWTYW0ULXHT</td>\n      <td>0321719824</td>\n      <td>{'Format:': ' DVD-ROM'}</td>\n      <td>Nate</td>\n      <td>The \"Learn by Video\" program for Flash CS5 wou...</td>\n      <td>A helpful introduction to Flash CS5 - a bit dr...</td>\n      <td>1290816000</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>4.0</td>\n      <td>False</td>\n      <td>10 21, 2010</td>\n      <td>A3QJU4FEN8PQSZ</td>\n      <td>0321719824</td>\n      <td>{'Format:': ' DVD-ROM'}</td>\n      <td>Grimmy</td>\n      <td>The presenter here sounds much more natural th...</td>\n      <td>A good introduction to Flash, with some UI wea...</td>\n      <td>1287619200</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>4.0</td>\n      <td>False</td>\n      <td>10 10, 2010</td>\n      <td>A31N0XY2UTB25C</td>\n      <td>0321719824</td>\n      <td>{'Format:': ' DVD-ROM'}</td>\n      <td>Stephanie Sullivan</td>\n      <td>This certified associate courseware comes as a...</td>\n      <td>Learn Adobe Flash Professional CS5 by Video</td>\n      <td>1286668800</td>\n      <td></td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 216
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def simularityScore(person1,person2):\n",
    "    # Movies and scores\n",
    "    u1Reviews = df.loc[df[\"reviewerID\"] == person1][['asin','overall']]\n",
    "    u2Reviews = df.loc[df[\"reviewerID\"] == person2][['asin','overall']]\n",
    "    \n",
    "    eDiff = 0\n",
    "    # looping through each movie\n",
    "    for movie in u1Reviews.values:\n",
    "        u1score = u1Reviews.loc[u1Reviews[\"asin\"] == movie[0]]['overall'].values[0]\n",
    "        \n",
    "        # if there is a match, then calculating the square difference\n",
    "        if movie[0] in u2Reviews['asin'].values:\n",
    "            u2score = u2Reviews.loc[u2Reviews[\"asin\"] == movie[0]]['overall'].values[0]\n",
    "            diff = u1score - u2score\n",
    "            eDiff += diff**2\n",
    "        # If the other haven't seen the same movie\n",
    "        else:\n",
    "            eDiff += u1score**2\n",
    "\n",
    "    # returning it\n",
    "    return math.sqrt(eDiff)\n",
    "\n",
    "def getRecomForUserByProd(user,product,numRec = 5):\n",
    "    # Others who reviewed the product\n",
    "    reviewed = df.loc[df[\"asin\"] == product][['reviewerID']]\n",
    "    reviewedProd = reviewed\n",
    "    \n",
    "    # Get similarity array of relevant people\n",
    "    simDict = []\n",
    "    for u2 in reviewedProd.values:\n",
    "        if user != u2:\n",
    "            simDict += [(u2, simularityScore(user, u2[0] ))]\n",
    "    \n",
    "    # Similarity array in order\n",
    "    simDict = sorted(simDict, key=lambda tup: tup[1])\n",
    "    \n",
    "    # Create recommendations\n",
    "    seenProducts = df.loc[df.reviewerID == user][['asin']].values\n",
    "    # The product needs to be good enough\n",
    "    minRate = 3.5\n",
    "    \n",
    "    recommendations = []\n",
    "    it = 0\n",
    "    while len(recommendations) < numRec and it < 5:\n",
    "        prods = df.loc[df.reviewerID == simDict[it][0][0] ][['asin','overall']].values\n",
    "        \n",
    "        for (asin,overall) in prods:\n",
    "            # Not bad and unseen product\n",
    "            if overall > minRate and asin not in seenProducts:\n",
    "                recommendations += [asin]\n",
    "            if len(recommendations) >= numRec:\n",
    "                break\n",
    "        it += 1\n",
    "    # Give recommendations\n",
    "    return recommendations\n",
    "\n",
    "def getRecomForUser(user,numRec = 5):\n",
    "    reviewers = df.reviewerID.drop_duplicates()\n",
    "\n",
    "    simDict = []\n",
    "    for u2 in reviewers.values:\n",
    "        if user != u2:\n",
    "            sim = simularityScore(user,u2)\n",
    "            simDict += [(u2,sim)]\n",
    "            # print(\"User1:\", user,\" User2:\", u2,\" sim:\",sim)\n",
    "    \n",
    "    # Similarity array in order\n",
    "    simDict = sorted(simDict, key = lambda tup: tup[1])\n",
    "    # print('simDict: ',simDict[:20])\n",
    "    # print('first: ',simDict[0])\n",
    "    \n",
    "    # Create recommendations\n",
    "    seenProducts = df.loc[df.reviewerID == user][['asin']].values\n",
    "    recommendations = []\n",
    "    minRate = 3.5\n",
    "    it = 0\n",
    "    while len(recommendations) < numRec and it < len(simDict):\n",
    "        prods = df.loc[df.reviewerID == simDict[it][0] ][['asin','overall']].values\n",
    "        \n",
    "        for (asin,overall) in prods:\n",
    "            # Not bad and unseen product\n",
    "            if overall > minRate and asin not in seenProducts:\n",
    "                recommendations += [asin]\n",
    "            if len(recommendations) >= numRec:\n",
    "                break\n",
    "        # it += 1\n",
    "    return recommendations"
   ]
  },
  {
   "source": [
    "## Calculate avarage number of ratings per user and product"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Avarage number of ratings per user: 7.0\n",
      "Avarage number of ratings per product: 16.0\n"
     ]
    }
   ],
   "source": [
    "# per user\n",
    "reviewers = df[\"reviewerID\"].drop_duplicates().values\n",
    "rs = 0\n",
    "for r in reviewers:\n",
    "    rs += len(df.loc[df.reviewerID == r ].values)\n",
    "print(\"Avarage number of ratings per user:\", round(rs / len(reviewers) ,0) )\n",
    "\n",
    "# per product\n",
    "products = df[\"asin\"].drop_duplicates().values\n",
    "pr = 0\n",
    "for p in products:\n",
    "    pr += len(df.loc[df.asin == p ].values)\n",
    "print(\"Avarage number of ratings per product:\", round(pr / len(products) ,0) )"
   ]
  },
  {
   "source": [
    "## Recommender System with Collaborative filtering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['B00NASFCDO', 'B003ZRTDPS', '0321700945', 'B000PC1IR2', 'B0017KEA6W']\n",
      "getRecomForUserByProd runtime: 0.09634\n",
      "['0321719824', 'B005LTV8G0', 'B008YTAGIK', 'B0148BYIPY', '0321719824']\n",
      "getRecomForUser runtime:  15.47598\n"
     ]
    }
   ],
   "source": [
    "# Simularity score check\n",
    "import time\n",
    "person1 = \"AYUF7YETYOLNX\"\n",
    "person2 = \"A3QJU4FEN8PQSZ\"\n",
    "product1 = \"0321700945\"\n",
    "\n",
    "# print(\"person1:\", person1, \"person2:\", person2, \"score:\", simularityScore(person1,person2))\n",
    "# print(\"person1:\", person1, \"person2:\", person2, \"score:\", simularityScore(person2,person1))\n",
    "\n",
    "# Recommending\n",
    "t1 = time.time()\n",
    "print(getRecomForUserByProd(person1,product1) )\n",
    "t2 = time.time()\n",
    "print(\"getRecomForUserByProd runtime:\", round(t2-t1,5))\n",
    "print(getRecomForUser(person1) )\n",
    "t3 = time.time()\n",
    "print(\"getRecomForUser runtime: \", round(t3-t2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df['reviewerID'] == 'A38NELQT98S4H8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df['asin'] == '0321719816']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install krippendorff"
   ]
  },
  {
   "source": [
    "# Crowdsourcing Exercise"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import krippendorff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# importing libraries\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CrowdSourcer():\n",
    "    \n",
    "    def __init__(self, sourcePath):\n",
    "        # Get data\n",
    "        self.data = pd.read_csv(sourcePath)\n",
    "\n",
    "        # The feature phrases removed (To separate rating)\n",
    "        cols = []\n",
    "        for i in range(29):\n",
    "            cols.append(\"feature\" + str(i + 1))\n",
    "\n",
    "        \n",
    "        self.ratings = self.data.drop([\"Sentence Index\",\"Ground Truth Labels\"] + cols, axis = 1)\n",
    "        self.groundTruth = self.data[\"Ground Truth Labels\"]\n",
    "        self.workers = self.ratings.columns\n",
    "        # features = data.drop([\"Sentence Index\",\"Ground Truth Labels\"] + workers.values[:], axis = 1)\n",
    "        # print(self.ratings)\n",
    "\n",
    "    def calcKrippendorff(self):\n",
    "        missing = \"\"        \n",
    "        kripCoef = krippendorff.alpha(reliability_data = self.ratings)\n",
    "        print(\"Krippendorff coeficient: %.3f\" % kripCoef)\n",
    "\n",
    "    # Generate the prediction matrix out of the global sheet data.\n",
    "    def trainByCS(self, X_train, y_train,voteType = \"hard\", solver = \"lbfgs\"):\n",
    "        # group / ensemble of models\n",
    "        self.lr = LogisticRegression(solver = solver, multi_class =\"multinomial\", max_iter = 200)\n",
    "        self.svc = SVC(gamma = \"auto\", probability = True)\n",
    "        self.dtc = DecisionTreeClassifier()\n",
    "        self.estimator = [(\"LR\", self.lr)),(\"SVC\", self.svc)),(\"DTC\", self.dtc)]\n",
    "        \n",
    "        # Voting Classifier with hard voting\n",
    "        self.voters = VotingClassifier(estimators = self.estimator, voting = voteType)\n",
    "        self.voters.fit(X_train, y_train)\n",
    "        \n",
    "\n",
    "    def checkPrediction(self,ratings,truths):\n",
    "        y_pred = self.voters.predict(ratings)\n",
    "        # print(whole)\n",
    "        misses = 0\n",
    "        for guess,truth in zip(y_pred, truths):\n",
    "            # print(guess,truth)\n",
    "            if guess != truth:\n",
    "                misses += 1\n",
    "        print(misses, \"miss(es) out of\",len(truths), \" accurary:\", str(100*round( 1-(misses/len(truths)) , 2)) + \"%\" )\n",
    "        return str(100*round( 1-(misses/len(truths)) , 2))\n",
    "        \n",
    "collabData = CrowdSourcer(\"GlobalSheet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Krippendorff coeficient: 0.005\n"
     ]
    }
   ],
   "source": [
    "collabData.calcKrippendorff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "size of train data: 9\n",
      "size of test data: 92.0\n",
      "3 miss(es) out of 92  accurary: 97.0%\n",
      "4 miss(es) out of 92  accurary: 96.0%\n",
      "4 miss(es) out of 92  accurary: 96.0%\n",
      "2 miss(es) out of 92  accurary: 98.0%\n",
      "4 miss(es) out of 92  accurary: 96.0%\n"
     ]
    }
   ],
   "source": [
    "# Training and testing the crowdsourcing\n",
    "testSize = 0.91\n",
    "\n",
    "print(\"size of train data:\", round( len(collabData.groundTruth)*(1-testSize)) )\n",
    "print(\"size of test data:\",  round( len(collabData.groundTruth)*testSize,0) )\n",
    "\n",
    "#### TESTING PARAMETERS ####\n",
    "# solvers = [\"newton-cg\",\"lbfgs\"]\n",
    "# for s in solvers:\n",
    "#     print(s)\n",
    "#     for j in range(2):\n",
    "#         vote = \"hard\"\n",
    "#         if j == 1:\n",
    "#             vote = \"soft\"\n",
    "#         print(\"with\", vote,\"vote\")\n",
    "#         for i in range(20):\n",
    "#             X_train, X_test, y_train, y_test = train_test_split(collabData.ratings,\n",
    "#                                                                 collabData.groundTruth,\n",
    "#                                                                 test_size = testSize,\n",
    "#                                                                 random_state = 42)\n",
    "\n",
    "#             # collabData.trainByCS(X_train,y_train,\"soft\")\n",
    "#             collabData.trainByCS(X_train,y_train,vote,s)\n",
    "#             collabData.checkPrediction(X_test,y_test)\n",
    "#############################\n",
    "\n",
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(collabData.ratings,\n",
    "                                                        collabData.groundTruth,\n",
    "                                                        test_size = testSize,\n",
    "                                                        random_state = 42)\n",
    "\n",
    "    collabData.trainByCS(X_train,y_train,\"hard\")\n",
    "    collabData.checkPrediction(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class Debug(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X):\n",
    "        # print(\"Shape\",X.shape)\n",
    "        self.shape = X.shape\n",
    "        # print(self.shape)\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "source": [
    "# Stanford Data-set (SST) - Rotten Tomatoes reviews\n",
    "## Recommender System"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# SKLearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Recommender(SentAnalyzer):\n",
    "    def __init__(self, sourcePath):\n",
    "        SentAnalyzer.__init__(self, sourcePath)\n",
    "        \n",
    "        self.cleanData = True # (Overright)\n",
    "        self.testSize = 0.5\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.texts,\n",
    "                                                                                self.labels,\n",
    "                                                                                test_size = self.testSize,\n",
    "                                                                                random_state = 42)\n",
    "\n",
    "    def getDocTermMtx(self,n_min = 1,n_max = 3):\n",
    "        self.cv = CountVectorizer(ngram_range=(n_min, n_max), lowercase = True)\n",
    "        if self.cleanData:\n",
    "            self.cv = CountVectorizer(ngram_range=(n_min, n_max), lowercase = True, preprocessor = self.cleanString)\n",
    "\n",
    "        self.cv.fit(self.texts.values)\n",
    "        self.wcVector = self.cv.transform(self.texts.values)\n",
    "\n",
    "        self.vocab = list(self.cv.vocabulary_.items())\n",
    "        print('Vocabulary len:',len(self.vocab))\n",
    "        print(\"Word Vector matrix:\",self.wcVector.shape)\n",
    "\n",
    "        # Unigram Tf-Idf\n",
    "        \n",
    "        self.tfidf = TfidfTransformer()\n",
    "        self.tfidf.fit(self.wcVector)\n",
    "\n",
    "        # Numericalize the train dataset\n",
    "        self.docTermMtx = self.tfidf.transform(self.wcVector)\n",
    "        print('Document-term matrix',self.docTermMtx.shape)\n",
    "        \n",
    "    def dimensionReduce(self,n):\n",
    "        svd = TruncatedSVD(n_components = n, n_iter=7, random_state=42)\n",
    "        svd.fit(self.docTermMtx)\n",
    "        self.redDocTermMtx = svd.transform(self.docTermMtx)\n",
    "\n",
    "        print(svd.explained_variance_ratio_)\n",
    "        print()\n",
    "        print(svd.explained_variance_ratio_.sum())\n",
    "        print()\n",
    "        print('Eigen values',svd.singular_values_)\n",
    "        print()\n",
    "        print('redDocTermMtx',self.redDocTermMtx)\n",
    "\n",
    "    def linReg(self):\n",
    "        self.lr = LogisticRegression(random_state = 0)\n",
    "        self.lrResult = self.lr.fit(self.X_train.values,self.y_train)\n",
    "\n",
    "    def withPipeline(self, n_min = 1, n_max = 1, svd_iter = 7, predictor = \"sgd\", useDimRed = True, nDim = 100):\n",
    "        start = time.time()\n",
    "\n",
    "        # Pipeline components\n",
    "        pipeArray = []\n",
    "        if self.cleanData:\n",
    "            self.cv = CountVectorizer(ngram_range=(n_min, n_max), lowercase = True, preprocessor = self.cleanString)\n",
    "            pipeArray.append((\"CV\", self.cv))\n",
    "        else:\n",
    "            self.cv = CountVectorizer(ngram_range=(n_min, n_max), lowercase = True)\n",
    "            pipeArray.append((\"CV\", self.cv))\n",
    "        \n",
    "        self.tfidf = TfidfTransformer()\n",
    "        pipeArray.append((\"TF-IDF\", self.tfidf))\n",
    "        \n",
    "        # Print doc-term-mtx info\n",
    "        self.debug = Debug()\n",
    "        pipeArray.append((\"Debug\", self.debug))\n",
    "        \n",
    "        if useDimRed:\n",
    "            self.svd = TruncatedSVD(n_components = nDim, n_iter = svd_iter, random_state = 42)\n",
    "            pipeArray.append((\"SVD\", self.svd))\n",
    "        \n",
    "        # Predictor used\n",
    "        if predictor == \"lr\":\n",
    "            self.lr = LogisticRegression(random_state = 0)\n",
    "            pipeArray.append((\"LR\", self.lr))\n",
    "        elif predictor == \"sgd\":\n",
    "            self.sgd = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "            pipeArray.append((\"SGD\", self.sgd))\n",
    "        \n",
    "        # Process pipeline\n",
    "        self.pipe = Pipeline(pipeArray)\n",
    "        self.result = self.pipe.fit(self.X_train.values, self.y_train)\n",
    "        end = time.time()\n",
    "        runtime = round(end - start, 2)\n",
    "\n",
    "        # Test\n",
    "        if useDimRed:\n",
    "            print('With dims:', nDim ,', tf-idf dims:', self.debug.shape , \"using\", predictor ,\", runtime:\",runtime)\n",
    "        else:\n",
    "            print('Without dim reduction, tf-idf dims:', self.debug.shape , \"using\", predictor ,\", runtime:\",runtime)\n",
    "        self.test()\n",
    "\n",
    "    def test(self, predicter = None):\n",
    "        if predicter == None:\n",
    "            predicter = self.pipe\n",
    "        self.pred = predicter.predict(self.X_test.values)\n",
    "        self.rightAns = 0\n",
    "        for pred, ans in zip(self.pred,self.y_test.values):\n",
    "            if pred == ans:\n",
    "                self.rightAns += 1\n",
    "        print('Right answers:',self.rightAns, 'of',len(self.pred),\n",
    "                'so', str(round(100*self.rightAns/len(self.pred),2)) + \"%\" )\n",
    "            \n",
    "    def predictWithModels(self):\n",
    "        self.model = Word2Vec.load(\"word2vec.model\")\n",
    "        self.model.train(self.X_train, total_examples=1, epochs=1)\n",
    "        print(\"train score:\", model.score(self.X_train, self.y_train))\n",
    "        print(\"test score:\", model.score(self.X_test, self.y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "sstRec = Recommender(\"stsa.binary.phrases.train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sstRec.getDocTermMtx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cleanData[:5].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()\n",
    "# sstRec.texts.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "sstRec.cleanData = False\n",
    "sstRec.withPipeline()\n",
    "# With dims: 20 , tf-idf dims: (38480, 14332) using LR, runtime 4.49\n",
    "# Right answers: 25438 of 38481 so 66.11%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "With dims: 100 , tf-idf dims: (38480, 14332) using lr , runtime: 5.31\n",
      "Right answers: 25960 of 38481 so 67.46%\n"
     ]
    }
   ],
   "source": [
    "sstRec.cleanData = False\n",
    "sstRec.withPipeline(predictor=\"lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "With dims: 100 , tf-idf dims: (38480, 14332) using sgd , runtime: 26.14\n",
      "Right answers: 24829 of 38481 so 64.52%\n"
     ]
    }
   ],
   "source": [
    "sstRec.cleanData = True\n",
    "sstRec.withPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "With dims: 200 , tf-idf dims: (38480, 175637) using sgd , runtime: 56.89\n",
      "Right answers: 27088 of 38481 so 70.39%\n"
     ]
    }
   ],
   "source": [
    "# SGD\n",
    "sstRec.cleanData = True\n",
    "sstRec.withPipeline(nDim=200,n_max=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "With dims: 200 , tf-idf dims: (38480, 14332) using lr , runtime: 29.58\n",
      "Right answers: 27248 of 38481 so 70.81%\n"
     ]
    }
   ],
   "source": [
    "# LR\n",
    "sstRec.cleanData = True\n",
    "sstRec.withPipeline(nDim=200,predictor=\"lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "With dims: 1000 , tf-idf dims: (38480, 175637) using lr , runtime: 261.97\n",
      "Right answers: 30117 of 38481 so 78.26%\n"
     ]
    }
   ],
   "source": [
    "# LR\n",
    "sstRec.cleanData = True\n",
    "sstRec.withPipeline(nDim=1000,predictor=\"lr\",n_max=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "sstRec.withPipeline(n_min = 1, n_max = 1) # 65.1%\n",
    "sstRec.withPipeline(n_min = 1, n_max = 2) # 64.15%\n",
    "sstRec.withPipeline(n_min = 1, n_max = 3) # 63.79%\n",
    "sstRec.withPipeline(n_min = 1, n_max = 4) # 63.71%\n",
    "end = time.time()\n",
    "print(\"runtime:\",round(end-start,2),\"msp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sstRec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sstRec.getDocTermMtx()\n",
    "sstRec.linReg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sstRec.dimensionReduce(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I tried the differences between \n",
    "# the CountVectorizer with different parameters\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Unigram Counts\n",
    "\n",
    "unigram_vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "unigram_vectorizer.fit(sstRec.texts.values)\n",
    "\n",
    "X_train_unigram = unigram_vectorizer.transform(sstRec.texts.values)\n",
    "\n",
    "# Unigram Tf-Idf\n",
    "unigram_tf_idf = TfidfTransformer()\n",
    "unigram_tf_idf.fit(X_train_unigram)\n",
    "\n",
    "X_train_unigram_tf_idf = unigram_tf_idf.transform(X_train_unigram)\n",
    "\n",
    "# trigram Counts\n",
    "trigram_vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "trigram_vectorizer.fit(sstRec.texts.values)\n",
    "\n",
    "X_train_trigram = trigram_vectorizer.transform(sstRec.texts.values)\n",
    "\n",
    "\n",
    "# trigram Tf-Idf\n",
    "trigram_tf_idf = TfidfTransformer()\n",
    "trigram_tf_idf.fit(X_train_trigram)\n",
    "\n",
    "X_train_trigram_tf_idf = trigram_tf_idf.transform(X_train_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "def train_and_show_scores(X, y, title) -> None:\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, train_size = 0.75, stratify = y\n",
    "    )\n",
    "\n",
    "    clf = SGDClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_score = clf.score(X_train, y_train)\n",
    "    valid_score = clf.score(X_valid, y_valid)\n",
    "    print(f'{title}\\nTrain score: {round(train_score, 2)} ; Validation score: {round(valid_score, 2)}\\n')\n",
    "\n",
    "y_train = sstRec.labels.values\n",
    "\n",
    "train_and_show_scores(X_train_unigram, y_train, 'Unigram Counts')\n",
    "train_and_show_scores(X_train_unigram_tf_idf, y_train, 'Unigram Tf-Idf')\n",
    "train_and_show_scores(X_train_trigram, y_train, 'Trigram Counts')\n",
    "train_and_show_scores(X_train_trigram_tf_idf, y_train, 'Trigram Tf-Idf')\n",
    "\n",
    "train_and_show_scores(X_train_unigram, y_train, 'Unigram Counts')\n",
    "train_and_show_scores(X_train_unigram_tf_idf, y_train, 'Unigram Tf-Idf')\n",
    "train_and_show_scores(X_train_trigram, y_train, 'Trigram Counts')\n",
    "train_and_show_scores(X_train_trigram_tf_idf, y_train, 'Trigram Tf-Idf')"
   ]
  }
 ]
}