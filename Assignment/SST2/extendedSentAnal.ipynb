{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de",
   "display_name": "Python 3.8.5 64-bit (conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "\n",
    "class SentAnalyzer():\n",
    "    def __init__(self, sourcePath):\n",
    "        self.data = pd.read_csv(sourcePath)\n",
    "        self.indexes = self.data['index']\n",
    "        self.labels = self.data['label']\n",
    "        self.texts = self.data['text']\n",
    "        self.textsL1 = self.data.loc[self.data['label'] == 1]\n",
    "        self.textsL0 = self.data.loc[self.data['label'] == 0]\n",
    "        \n",
    "        # self.vectorizer = TfidfVectorizer()\n",
    "        # self.word_count_vector = self.cv.fit_transform(self.texts)\n",
    "    \n",
    "    ##########\n",
    "    ## SST2 ##\n",
    "    ##########\n",
    "    \n",
    "    def checkBalance(self):\n",
    "        len1 = len(self.textsL1)\n",
    "        len0 = len(self.textsL0)\n",
    "        print('Label 1:',len1)\n",
    "        print('Label 0:',len0)\n",
    "        if len1 > len0:\n",
    "            print('Their ratio:',abs(len0/len1))\n",
    "        else:\n",
    "            print('Their ratio:',abs(len1/len0))\n",
    "            \n",
    "\n",
    "    def nGrams(self, minN,maxN):\n",
    "        for i in range(minN,maxN):\n",
    "            cv = CountVectorizer(ngram_range=(i,i))\n",
    "            # word_count_vector = cv.fit_transform(self.texts)\n",
    "            word_count_vector = cv.fit_transform(self.texts)\n",
    "            # print(word_count_vector)\n",
    "            \n",
    "            tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "            # tfidf_transformer=TfidfTransformer()\n",
    "            tfidf_transformer.fit(word_count_vector)\n",
    "            \n",
    "            df_idf = pd.DataFrame(tfidf_transformer.idf_, index = cv.get_feature_names(),columns=[\"idf_weights\"])\n",
    "            \n",
    "            # sort ascending \n",
    "            print(df_idf.sort_values(by = ['idf_weights'])[:5])\n",
    "    \n",
    "    def lengthCorrelation(self):\n",
    "        lengths = [(lambda x: len(x[1]))(x) for x in self.texts.items()]\n",
    "        lenLab = pd.DataFrame( list(zip(lengths, self.labels)), columns = [\"length\",\"labels\"])\n",
    "        print(lenLab.corr())\n",
    "    \n",
    "    def tfIdfWithCV(self):\n",
    "        word_count_vector = CountVectorizer().fit_transform(self.texts)\n",
    "        print(word_count_vector)\n",
    "        wc = pd.DataFrame(word_count_vector, index = cv.get_feature_names(), columns = [\"idf_weights\"])\n",
    "        # sort ascending \n",
    "        # df_idf = pd.DataFrame(word_count_vector.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n",
    "    \n",
    "    def tfIdfWithTranformer(self):\n",
    "        # this steps generates word counts for the words in your docs \n",
    "        cv = CountVectorizer()\n",
    "        word_count_vector = cv.fit_transform(self.texts)\n",
    "        # print(word_count_vector)\n",
    "        \n",
    "        tfidf_transformer=TfidfTransformer(smooth_idf = True, use_idf = True) \n",
    "        tfidf_transformer.fit(word_count_vector)\n",
    "        \n",
    "        df_idf = pd.DataFrame(tfidf_transformer.idf_, index = cv.get_feature_names(), columns = [\"idf_weights\"])\n",
    "        \n",
    "        # sort ascending \n",
    "        # print(self.tfidf_transformer.idf_)\n",
    "        print(df_idf.sort_values(by = ['idf_weights']))\n",
    "\n",
    "    def majorityVote(self):\n",
    "        estimators = []\n",
    "        estimator.append(('LR', \n",
    "                  LogisticRegression(solver ='lbfgs', \n",
    "                                     multi_class ='multinomial', \n",
    "                                     max_iter = 200)))\n",
    "        estimator.append(('SVC', SVC(gamma ='auto', probability = True)))\n",
    "        estimator.append(('DTC', DecisionTreeClassifier()))\n",
    "\n",
    "    def collabEval(self):\n",
    "        # Get data\n",
    "        df = pd.read_csv(\"GlobalSheet.csv\")\n",
    "        \n",
    "        cols = []\n",
    "        for i in range(29):\n",
    "            cols.append(str(i+1))\n",
    "\n",
    "        ratings = df.drop([\"Sentence Index\",\"Ground Truth Labels\"] + cols,axis=1)\n",
    "        groundTruth = df[\"Ground Truth Labels\"]\n",
    "        users = ratings.columns\n",
    "        \n",
    "        # Start to calculate\n",
    "\n",
    "\n",
    "train = SentAnalyzer(\"stsa.binary.phrases.train\")\n",
    "#instantiate CountVectorizer() \n",
    "# train.tfIdfWithTranformer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print idf values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Label 1: 42259\nLabel 0: 34702\nTheir ratio: 0.8211741877469888\n"
     ]
    }
   ],
   "source": [
    "train.checkBalance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     idf_weights\n",
      "the     2.210299\n",
      "and     2.367068\n",
      "of      2.490469\n",
      "to      2.807791\n",
      "is      3.125087\n",
      "          idf_weights\n",
      "of the       4.053253\n",
      "in the       4.658701\n",
      "the film     4.880207\n",
      "to the       5.155174\n",
      "to be        5.196617\n",
      "              idf_weights\n",
      "one of the       6.096209\n",
      "the film is      6.918348\n",
      "the kind of      7.009320\n",
      "the movie is     7.086281\n",
      "of the year      7.103573\n"
     ]
    }
   ],
   "source": [
    "# Get the n-grams\n",
    "train.nGrams(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "          length    labels\nlength  1.000000 -0.037769\nlabels -0.037769  1.000000\n"
     ]
    }
   ],
   "source": [
    "train.lengthCorrelation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape (12805, 12)\nColunm Name :  overall\nColumn Contents :  [4. 4. 5. 5. 5.]\nColunm Name :  verified\nColumn Contents :  [False False False False False]\nColunm Name :  reviewTime\nColumn Contents :  ['10 20, 2010' '10 18, 2010' '10 16, 2010' '10 12, 2010' '10 7, 2010']\nColunm Name :  reviewerID\nColumn Contents :  ['A38NELQT98S4H8' 'A3QJU4FEN8PQSZ' 'ACJT8MUC0LRF0' 'AYUF7YETYOLNX'\n 'A31ICLWQ9CSHRS']\nColunm Name :  asin\nColumn Contents :  ['0321719816' '0321719816' '0321719816' '0321719816' '0321719816']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from urllib.request import urlopen\n",
    "\n",
    "###################\n",
    "## Amazon Review ##\n",
    "###################\n",
    "\n",
    "# Load in the Amazon Review Data with 5-core\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('Software_5.json.gz')\n",
    "df = df.fillna('')\n",
    "print('shape', df.shape)\n",
    "i = 0\n",
    "for (columnName, columnData) in df.iteritems():\n",
    "    i+=1\n",
    "    if i > 5:\n",
    "        break\n",
    "    print('Colunm Name : ', columnName)\n",
    "    print('Column Contents : ', columnData.values[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting krippendorff\n  Downloading krippendorff-0.4.0-py3-none-any.whl (17 kB)\nRequirement already satisfied: numpy in c:\\programdata\\miniconda3\\lib\\site-packages (from krippendorff) (1.19.2)\nInstalling collected packages: krippendorff\nSuccessfully installed krippendorff-0.4.0\nNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install krippendorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nominal_metric(a, b):\n",
    "    return a != b\n",
    "\n",
    "\n",
    "def interval_metric(a, b):\n",
    "    return (a-b)**2\n",
    "\n",
    "\n",
    "def ratio_metric(a, b):\n",
    "    return ((a-b)/(a+b))**2\n",
    "\n",
    "def krippendorff_alpha(data, metric=interval_metric, force_vecmath=False, convert_items=float, missing_items=None):\n",
    "    '''\n",
    "    Calculate Krippendorff's alpha (inter-rater reliability):\n",
    "    \n",
    "    data is in the format\n",
    "    [\n",
    "        {unit1:value, unit2:value, ...},  # coder 1\n",
    "        {unit1:value, unit3:value, ...},   # coder 2\n",
    "        ...                            # more coders\n",
    "    ]\n",
    "    or \n",
    "    it is a sequence of (masked) sequences (list, numpy.array, numpy.ma.array, e.g.) with rows corresponding to coders and columns to items\n",
    "    \n",
    "    metric: function calculating the pairwise distance\n",
    "    force_vecmath: force vector math for custom metrics (numpy required)\n",
    "    convert_items: function for the type conversion of items (default: float)\n",
    "    missing_items: indicator for missing items (default: None)\n",
    "    '''\n",
    "    \n",
    "    # number of coders\n",
    "    m = len(data)\n",
    "    \n",
    "    # set of constants identifying missing values\n",
    "    if missing_items is None:\n",
    "        maskitems = []\n",
    "    else:\n",
    "        maskitems = list(missing_items)\n",
    "    if np is not None:\n",
    "        maskitems.append(np.ma.masked_singleton)\n",
    "    \n",
    "    # convert input data to a dict of items\n",
    "    units = {}\n",
    "    for d in data:\n",
    "        try:\n",
    "            # try if d behaves as a dict\n",
    "            diter = d.items()\n",
    "        except AttributeError:\n",
    "            # sequence assumed for d\n",
    "            diter = enumerate(d)\n",
    "            \n",
    "        for it, g in diter:\n",
    "            if g not in maskitems:\n",
    "                try:\n",
    "                    its = units[it]\n",
    "                except KeyError:\n",
    "                    its = []\n",
    "                    units[it] = its\n",
    "                its.append(convert_items(g))\n",
    "\n",
    "\n",
    "    units = dict((it, d) for it, d in units.items() if len(d) > 1)  # units with pairable values\n",
    "    n = sum(len(pv) for pv in units.values())  # number of pairable values\n",
    "    \n",
    "    if n == 0:\n",
    "        raise ValueError(\"No items to compare.\")\n",
    "    \n",
    "    np_metric = (np is not None) and ((metric in (interval_metric, nominal_metric, ratio_metric)) or force_vecmath)\n",
    "    \n",
    "    Do = 0.\n",
    "    for grades in units.values():\n",
    "        if np_metric:\n",
    "            gr = np.asarray(grades)\n",
    "            Du = sum(np.sum(metric(gr, gri)) for gri in gr)\n",
    "        else:\n",
    "            Du = sum(metric(gi, gj) for gi in grades for gj in grades)\n",
    "        Do += Du/float(len(grades)-1)\n",
    "    Do /= float(n)\n",
    "\n",
    "    if Do == 0:\n",
    "        return 1.\n",
    "\n",
    "    De = 0.\n",
    "    for g1 in units.values():\n",
    "        if np_metric:\n",
    "            d1 = np.asarray(g1)\n",
    "            for g2 in units.values():\n",
    "                De += sum(np.sum(metric(d1, gj)) for gj in g2)\n",
    "        else:\n",
    "            for g2 in units.values():\n",
    "                De += sum(metric(gi, gj) for gi in g1 for gj in g2)\n",
    "    De /= float(n*(n-1))\n",
    "\n",
    "    return 1.-Do/De if (Do and De) else 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Krippendorff metric: 0.005\n"
     ]
    }
   ],
   "source": [
    "# Crowdsourcing Exercise\n",
    "import krippendorff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def nominal_metric(a, b):\n",
    "    return a != b\n",
    "\n",
    "\n",
    "def interval_metric(a, b):\n",
    "    return (a-b)**2\n",
    "\n",
    "class crowdSourcer():\n",
    "    \n",
    "    def __init__(self, sourcePath):\n",
    "        self.data = pd.read_csv(sourcePath)\n",
    "        # data = pd.read_csv(sourcePath,header=1)\n",
    "\n",
    "        # The feature phrases removed\n",
    "        cols = []\n",
    "        for i in range(29):\n",
    "            cols.append(\"feature\"+str(i+1))\n",
    "        # print(cols)\n",
    "\n",
    "        \n",
    "        self.ratings = self.data.drop([\"Sentence Index\",\"Ground Truth Labels\"] + cols,axis=1)\n",
    "        self.groundTruth = self.data[\"Ground Truth Labels\"]\n",
    "        self.users = self.ratings.columns\n",
    "        # features = data.drop([\"Sentence Index\",\"Ground Truth Labels\"] + users.values[:], axis = 1)\n",
    "        # print(self.ratings)\n",
    "\n",
    "    def calcKrippendorff(self):\n",
    "        missing = ''        \n",
    "        # print(ratings)\n",
    "        kripCoef = krippendorff.alpha(reliability_data=self.ratings)\n",
    "        # kripNominalCoef = krippendorff.alpha(reliability_data=self.ratings, metric=nominal_metric, missing_items=missing)\n",
    "        # kripIntervalCoef = krippendorff.alpha(reliability_data=self.ratings, metric=interval_metric, missing_items=missing)\n",
    "        print(\"Krippendorff metric: %.3f\" % kripCoef)\n",
    "        \n",
    "\n",
    "collabData = crowdSourcer(\"GlobalSheet.csv\")\n",
    "collabData.calcKrippendorff()"
   ]
  }
 ]
}