{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de",
   "display_name": "Python 3.8.5 64-bit (conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "##########\n",
    "## SST2 ##\n",
    "##########\n",
    "\n",
    "class SentAnalyzer():\n",
    "    def __init__(self, sourcePath):\n",
    "        self.data = pd.read_csv(sourcePath)\n",
    "        self.indexes = self.data['index']\n",
    "        self.labels = self.data['label']\n",
    "        self.texts = self.data['text']\n",
    "        self.textsL1 = self.data.loc[self.data['label'] == 1][\"text\"]\n",
    "        self.textsL0 = self.data.loc[self.data['label'] == 0][\"text\"]\n",
    "    \n",
    "    def checkBalance(self):\n",
    "        len1 = len(self.textsL1)\n",
    "        len0 = len(self.textsL0)\n",
    "        print('Label 1:',len1)\n",
    "        print('Label 0:',len0)\n",
    "        if len1 > len0:\n",
    "            print('Their ratio:',round(abs(len1/len0),2))\n",
    "        else:\n",
    "            print('Their ratio:',round(abs(len0/len1),2))\n",
    "\n",
    "    def nGrams(self, data, minN,maxN,n):\n",
    "        for i in range(minN,maxN):\n",
    "            cv = CountVectorizer(ngram_range = (i,i))\n",
    "            word_count_vector = cv.fit_transform(data)\n",
    "            # print(word_count_vector)\n",
    "            \n",
    "            # Fit the model into the data\n",
    "            tfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "            tfidf_transformer.fit(word_count_vector)\n",
    "            \n",
    "            df_idf = pd.DataFrame(tfidf_transformer.idf_, index = cv.get_feature_names(),columns = [\"tfidf\"])\n",
    "            \n",
    "            # sort ascending \n",
    "            print(df_idf.sort_values(by = ['tfidf'])[:n])\n",
    "    \n",
    "    def lengthCorrelation(self):\n",
    "        lengths = [(lambda x: len(x[1]))(x) for x in self.texts.items()]\n",
    "        lenLab = pd.DataFrame( list(zip(lengths, self.labels)), columns = [\"length\",\"labels\"])\n",
    "        print(lenLab.corr())\n",
    "\n",
    "    def textTfidfValues(self,data):\n",
    "        # this steps generates word counts for the words in your docs \n",
    "        cv = CountVectorizer()\n",
    "        word_count_vector = cv.fit_transform(data)\n",
    "\n",
    "        # print(word_count_vector)\n",
    "        \n",
    "        tfidf_transformer = TfidfTransformer(smooth_idf = True, use_idf = True)\n",
    "        tfidf_transformer.fit(word_count_vector)\n",
    "        \n",
    "        self.tfidf = pd.DataFrame(tfidf_transformer.idf_, index = cv.get_feature_names(), columns = [\"tfidf\"])\n",
    "        \n",
    "        # sort ascending \n",
    "        # print(self.tfidf_transformer.idf_)\n",
    "        return self.tfidf.sort_values(by = [\"tfidf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Label 1: 42259\nLabel 0: 34702\nTheir ratio: 1.22\n"
     ]
    }
   ],
   "source": [
    "# Load the SST data\n",
    "train = SentAnalyzer(\"stsa.binary.phrases.train\")\n",
    "train.checkBalance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The 5 most common word overall and within the two label.\n",
      "Overall:\n",
      "        tfidf\n",
      "the  2.210299\n",
      "and  2.367068\n",
      "of   2.490469\n",
      "to   2.807791\n",
      "is   3.125087\n",
      "Label 0:\n",
      "        tfidf\n",
      "the  2.188806\n",
      "and  2.463642\n",
      "of   2.493857\n",
      "to   2.670419\n",
      "is   3.045642\n",
      "Label 1:\n",
      "        tfidf\n",
      "the  2.228244\n",
      "and  2.294130\n",
      "of   2.487614\n",
      "to   2.936536\n",
      "is   3.195215\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "print(\"The\", n ,\"most common word overall and within the two label.\")\n",
    "print(\"Overall:\")\n",
    "print(train.textTfidfValues(train.texts)[:n])\n",
    "print(\"Label 0:\")\n",
    "print(train.textTfidfValues(train.textsL0).iloc[:n])\n",
    "print(\"Label 1:\")\n",
    "print(train.textTfidfValues(train.textsL1).iloc[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Whole database\n",
      "        tfidf\n",
      "the  2.210299\n",
      "and  2.367068\n",
      "of   2.490469\n",
      "to   2.807791\n",
      "is   3.125087\n",
      "             tfidf\n",
      "of the    4.053253\n",
      "in the    4.658701\n",
      "the film  4.880207\n",
      "to the    5.155174\n",
      "to be     5.196617\n",
      "                 tfidf\n",
      "one of the    6.096209\n",
      "the film is   6.918348\n",
      "the kind of   7.009320\n",
      "the movie is  7.086281\n",
      "of the year   7.103573\n"
     ]
    }
   ],
   "source": [
    "# Get the n-grams\n",
    "print(\"Whole database\")\n",
    "train.nGrams(train.texts, 1, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Label 1 sentences\n",
      "        tfidf\n",
      "the  2.228244\n",
      "and  2.294130\n",
      "of   2.487614\n",
      "to   2.936536\n",
      "is   3.195215\n",
      "             tfidf\n",
      "of the    3.997627\n",
      "in the    4.694099\n",
      "the film  4.920578\n",
      "to the    5.166961\n",
      "and the   5.273170\n",
      "                tfidf\n",
      "one of the   5.771063\n",
      "the film is  6.724343\n",
      "of the year  6.807409\n",
      "of the most  6.906664\n",
      "of the best  6.933097\n"
     ]
    }
   ],
   "source": [
    "# Get the n-grams of Label 1\n",
    "print(\"Label 1 sentences\")\n",
    "train.nGrams(train.textsL1, 1, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Label 0 sentences\n",
      "        tfidf\n",
      "the  2.188806\n",
      "and  2.463642\n",
      "of   2.493857\n",
      "to   2.670419\n",
      "is   3.045642\n",
      "              tfidf\n",
      "of the     4.124832\n",
      "in the     4.616176\n",
      "the film   4.831845\n",
      "to be      5.060991\n",
      "the movie  5.115987\n",
      "                 tfidf\n",
      "the movie is  6.610394\n",
      "one of the    6.718383\n",
      "of the film   6.900705\n",
      "the kind of   7.097873\n",
      "of its own    7.137093\n"
     ]
    }
   ],
   "source": [
    "# Get the n-grams of Label 0\n",
    "print(\"Label 0 sentences\")\n",
    "train.nGrams(train.textsL0, 1, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "          length    labels\nlength  1.000000 -0.037769\nlabels -0.037769  1.000000\n"
     ]
    }
   ],
   "source": [
    "train.lengthCorrelation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape (12805, 12)\nColunm Name :  overall\nColumn Contents :  [4. 4. 5. 5. 5.]\nColunm Name :  verified\nColumn Contents :  [False False False False False]\nColunm Name :  reviewTime\nColumn Contents :  ['10 20, 2010' '10 18, 2010' '10 16, 2010' '10 12, 2010' '10 7, 2010']\nColunm Name :  reviewerID\nColumn Contents :  ['A38NELQT98S4H8' 'A3QJU4FEN8PQSZ' 'ACJT8MUC0LRF0' 'AYUF7YETYOLNX'\n 'A31ICLWQ9CSHRS']\nColunm Name :  asin\nColumn Contents :  ['0321719816' '0321719816' '0321719816' '0321719816' '0321719816']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from urllib.request import urlopen\n",
    "\n",
    "###################\n",
    "## Amazon Review ##\n",
    "###################\n",
    "\n",
    "# Load in the Amazon Review Data with 5-core\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('Software_5.json.gz')\n",
    "df = df.fillna('')\n",
    "print('shape', df.shape)\n",
    "i = 0\n",
    "for (columnName, columnData) in df.iteritems():\n",
    "    i+=1\n",
    "    if i > 5:\n",
    "        break\n",
    "    print('Colunm Name : ', columnName)\n",
    "    print('Column Contents : ', columnData.values[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install krippendorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crowdsourcing Exercise\n",
    "import krippendorff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# importing libraries\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CrowdSourcer():\n",
    "    \n",
    "    def __init__(self, sourcePath):\n",
    "        # Get data\n",
    "        self.data = pd.read_csv(sourcePath)\n",
    "\n",
    "        # The feature phrases removed (To separate rating)\n",
    "        cols = []\n",
    "        for i in range(29):\n",
    "            cols.append(\"feature\"+str(i+1))\n",
    "\n",
    "        \n",
    "        self.ratings = self.data.drop([\"Sentence Index\",\"Ground Truth Labels\"] + cols,axis=1)\n",
    "        self.groundTruth = self.data[\"Ground Truth Labels\"]\n",
    "        self.workers = self.ratings.columns\n",
    "        # features = data.drop([\"Sentence Index\",\"Ground Truth Labels\"] + workers.values[:], axis = 1)\n",
    "        # print(self.ratings)\n",
    "\n",
    "    def calcKrippendorff(self):\n",
    "        missing = \"\"        \n",
    "        kripCoef = krippendorff.alpha(reliability_data = self.ratings)\n",
    "        print(\"Krippendorff coeficient: %.3f\" % kripCoef)\n",
    "\n",
    "    # Generate the prediction matrix out of the global sheet data.\n",
    "    def trainByCS(self, X_train, y_train,voteType = \"hard\", solver = \"lbfgs\"):\n",
    "        # group / ensemble of models\n",
    "        self.estimator = []\n",
    "        self.estimator.append((\"LR\", \n",
    "                        LogisticRegression(solver = solver, \n",
    "                                            multi_class =\"multinomial\", \n",
    "                                            max_iter = 200)))\n",
    "        self.estimator.append((\"SVC\", SVC(gamma = \"auto\", probability = True)))\n",
    "        self.estimator.append((\"DTC\", DecisionTreeClassifier()))\n",
    "\n",
    "        # Voting Classifier with hard voting\n",
    "        self.voters = VotingClassifier(estimators = self.estimator, voting = voteType)\n",
    "        self.voters.fit(X_train, y_train)\n",
    "        \n",
    "\n",
    "    def checkPrediction(self,ratings,truths):\n",
    "        y_pred = self.voters.predict(ratings)\n",
    "        # print(whole)\n",
    "        misses = 0\n",
    "        for guess,truth in zip(y_pred, truths):\n",
    "            # print(guess,truth)\n",
    "            if guess != truth:\n",
    "                misses += 1\n",
    "        print(misses, \"miss(es) out of\",len(truths), \" accurary:\", str(100*round( 1-(misses/len(truths)) , 2)) + \"%\" )\n",
    "        return str(100*round( 1-(misses/len(truths)) , 2))\n",
    "        \n",
    "\n",
    "collabData = CrowdSourcer(\"GlobalSheet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Krippendorff coeficient: 0.005\n"
     ]
    }
   ],
   "source": [
    "collabData.calcKrippendorff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "size of train data: 9\n",
      "size of test data: 92.0\n",
      "4 miss(es) out of 92  accurary: 96.0%\n",
      "4 miss(es) out of 92  accurary: 96.0%\n",
      "4 miss(es) out of 92  accurary: 96.0%\n",
      "4 miss(es) out of 92  accurary: 96.0%\n",
      "4 miss(es) out of 92  accurary: 96.0%\n"
     ]
    }
   ],
   "source": [
    "# Training and testing the crowdsourcing\n",
    "testSize = 0.91\n",
    "\n",
    "print(\"size of train data:\", round( len(collabData.groundTruth)*(1-testSize)) )\n",
    "print(\"size of test data:\",  round( len(collabData.groundTruth)*testSize,0) )\n",
    "\n",
    "#### TESTING PARAMETERS ####\n",
    "# solvers = [\"newton-cg\",\"lbfgs\"]\n",
    "# for s in solvers:\n",
    "#     print(s)\n",
    "#     for j in range(2):\n",
    "#         vote = \"hard\"\n",
    "#         if j == 1:\n",
    "#             vote = \"soft\"\n",
    "#         print(\"with\", vote,\"vote\")\n",
    "#         for i in range(20):\n",
    "#             X_train, X_test, y_train, y_test = train_test_split(collabData.ratings,\n",
    "#                                                                 collabData.groundTruth,\n",
    "#                                                                 test_size = testSize,\n",
    "#                                                                 random_state = 42)\n",
    "\n",
    "#             # collabData.trainByCS(X_train,y_train,\"soft\")\n",
    "#             collabData.trainByCS(X_train,y_train,vote,s)\n",
    "#             collabData.checkPrediction(X_test,y_test)\n",
    "#############################\n",
    "\n",
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(collabData.ratings,\n",
    "                                                        collabData.groundTruth,\n",
    "                                                        test_size = testSize,\n",
    "                                                        random_state = 42)\n",
    "\n",
    "    collabData.trainByCS(X_train,y_train,\"hard\")\n",
    "    collabData.checkPrediction(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "start\ndone\n"
     ]
    }
   ],
   "source": [
    "# SKLearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.parsing.preprocessing import *\n",
    "import gensim.downloader\n",
    "\n",
    "class Recommender(SentAnalyzer):\n",
    "    def __init__(self, sourcePath):\n",
    "        SentAnalyzer.__init__(self, sourcePath)\n",
    "        self.testSize = 0.5\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.texts,\n",
    "                                                                                self.labels,\n",
    "                                                                                test_size = testSize,\n",
    "                                                                                random_state = 42)\n",
    "\n",
    "    def tokenizeData(self):\n",
    "        # self.cd = self.clearData(self.texts)\n",
    "        # !!!Might wanna preprocess the data!!!\n",
    "        self.cv = CountVectorizer(decode_error='ignore', lowercase=True, min_df=2)\n",
    "        # self.cv.fit(self.X_train)\n",
    "        \n",
    "        # self.wcVector = self.cv.fit_transform(self.X_train.values.astype('U'))\n",
    "        # print(\"self.wcVector.shape\",self.wcVector.shape)\n",
    "\n",
    "        # self.vocab = list(self.cv.vocabulary_.items())\n",
    "        # print('self.cv.vocabulary len:',len(self.vocab))\n",
    "\n",
    "        # self.cv = CountVectorizer(ngram_range=(1, 1))\n",
    "        self.cv.fit(self.texts.values)\n",
    "\n",
    "        self.wcVector = self.cv.transform(self.texts.values)\n",
    "        # Unigram Tf-Idf\n",
    "        \n",
    "        self.tfidf = TfidfTransformer()\n",
    "        self.tfidf.fit(self.wcVector)\n",
    "\n",
    "        self.docTermMtx = self.tfidf.transform(self.wcVector)\n",
    "\n",
    "\n",
    "    def genSentenceMatrix(self):\n",
    "        # print(self.X_train.values.astype('U'))\n",
    "        print(self.X_train.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # Create the numericalizer TFIDF for lowercase\n",
    "        # self.tfidf = TfidfVectorizer(decode_error='ignore', lowercase = True, min_df=2)\n",
    "        # self.docTermMtx = self.tfidf.fit_transform(self.X_train)\n",
    "        # self.tfidf = TfidfTransformer()\n",
    "        # self.tfidf.fit(self.X_train)\n",
    "\n",
    "        # self.docTermMtx = self.tfidf.transform(self.X_train)\n",
    "\n",
    "        # Numericalize the train dataset\n",
    "        self.docTermMtx = self.tfidf.fit_transform(self.wcVector)\n",
    "        \n",
    "        # Numericalize the test dataset\n",
    "        self.test = self.tfidf.transform(self.X_train.values.astype('U'))\n",
    "        \n",
    "        print('Document-term matrix',self.docTermMtx.shape)\n",
    "        # print('tfidf.idf_',self.tfidf.idf_.shape)\n",
    "\n",
    "        # self.pca.fit(self.wcVector.toarray())\n",
    "        \n",
    "    def dimensionReduce(self):\n",
    "        self.genSentenceMatrix()\n",
    "        \n",
    "        svd = TruncatedSVD(n_components = 100, n_iter=7, random_state=42)\n",
    "        svd.fit(self.docTermMtx)\n",
    "\n",
    "        print(svd.explained_variance_ratio_)\n",
    "        print()\n",
    "        print(svd.explained_variance_ratio_.sum())\n",
    "        print()\n",
    "        print('Eigen values',svd.singular_values_)\n",
    "\n",
    "    def predictWithModels(self):\n",
    "        self.model = Word2Vec.load(\"word2vec.model\")\n",
    "        self.model.train(self.X_train, total_examples=1, epochs=1)\n",
    "        # print(\"train score:\", model.score(self.X_train, self.y_train))\n",
    "        # print(\"test score:\", model.score(self.X_test, self.y_test))\n",
    "\n",
    "print(\"start\")\n",
    "sstRec = Recommender(\"stsa.binary.phrases.train\")\n",
    "# sstRec.predictWithModels()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sstRec.tokenizeData()\n",
    "# sstRec.genSentenceMatrix()\n",
    "# sstRec.dimensionReduce()\n",
    "# sstRec.predictWithModels()\n",
    "# sstRec.pipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I tried the differences between \n",
    "# the CountVectorizer with different parameters\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Unigram Counts\n",
    "\n",
    "unigram_vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "unigram_vectorizer.fit(sstRec.texts.values)\n",
    "\n",
    "X_train_unigram = unigram_vectorizer.transform(sstRec.texts.values)\n",
    "\n",
    "# Unigram Tf-Idf\n",
    "unigram_tf_idf = TfidfTransformer()\n",
    "unigram_tf_idf.fit(X_train_unigram)\n",
    "\n",
    "X_train_unigram_tf_idf = unigram_tf_idf.transform(X_train_unigram)\n",
    "\n",
    "# Bigram Counts\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "bigram_vectorizer.fit(sstRec.texts.values)\n",
    "\n",
    "X_train_bigram = bigram_vectorizer.transform(sstRec.texts.values)\n",
    "\n",
    "\n",
    "# Bigram Tf-Idf\n",
    "bigram_tf_idf = TfidfTransformer()\n",
    "bigram_tf_idf.fit(X_train_bigram)\n",
    "\n",
    "X_train_bigram_tf_idf = bigram_tf_idf.transform(X_train_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unigram Counts\n",
      "Train score: 0.92 ; Validation score: 0.88\n",
      "\n",
      "Unigram Tf-Idf\n",
      "Train score: 0.9 ; Validation score: 0.87\n",
      "\n",
      "Bigram Counts\n",
      "Train score: 0.97 ; Validation score: 0.91\n",
      "\n",
      "Bigram Tf-Idf\n",
      "Train score: 0.92 ; Validation score: 0.88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "def train_and_show_scores(X: csr_matrix, y: np.array, title: str) -> None:\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, train_size=0.75, stratify=y\n",
    "    )\n",
    "\n",
    "    clf = SGDClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_score = clf.score(X_train, y_train)\n",
    "    valid_score = clf.score(X_valid, y_valid)\n",
    "    print(f'{title}\\nTrain score: {round(train_score, 2)} ; Validation score: {round(valid_score, 2)}\\n')\n",
    "\n",
    "y_train = sstRec.labels.values\n",
    "\n",
    "train_and_show_scores(X_train_unigram, y_train, 'Unigram Counts')\n",
    "train_and_show_scores(X_train_unigram_tf_idf, y_train, 'Unigram Tf-Idf')\n",
    "train_and_show_scores(X_train_bigram, y_train, 'Bigram Counts')\n",
    "train_and_show_scores(X_train_bigram_tf_idf, y_train, 'Bigram Tf-Idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "X_train = X_train_bigram_tf_idf\n",
    "\n",
    "# Phase 1: loss, learning rate and initial learning rate\n",
    "\n",
    "clf = SGDClassifier()\n",
    "\n",
    "distributions = dict(\n",
    "    loss=['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "    learning_rate=['optimal', 'invscaling', 'adaptive'],\n",
    "    eta0=uniform(loc=1e-7, scale=1e-2)\n",
    ")\n",
    "\n",
    "random_search_cv = RandomizedSearchCV(\n",
    "    estimator=clf,\n",
    "    param_distributions=distributions,\n",
    "    cv=5,\n",
    "    n_iter=50\n",
    ")\n",
    "random_search_cv.fit(sstRec.X_train, sstRec.y_train)\n",
    "print(f'Best params: {random_search_cv.best_params_}')\n",
    "print(f'Best score: {random_search_cv.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "# Phase 2: penalty and alpha\n",
    "\n",
    "clf = SGDClassifier()\n",
    "\n",
    "distributions = dict(\n",
    "    penalty=['l1', 'l2', 'elasticnet'],\n",
    "    alpha=uniform(loc=1e-6, scale=1e-4)\n",
    ")\n",
    "\n",
    "random_search_cv = RandomizedSearchCV(\n",
    "    estimator=clf,\n",
    "    param_distributions=distributions,\n",
    "    cv=5,\n",
    "    n_iter=50\n",
    ")\n",
    "random_search_cv.fit(sstRec.X_train, sstRec.y_train)\n",
    "print(f'Best params: {random_search_cv.best_params_}')\n",
    "print(f'Best score: {random_search_cv.best_score_}')\n",
    "\n",
    "sgd_classifier = random_search_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'sgd_classifier' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-159-1acdc99acb86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msstRec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msgd_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sgd_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "X_test = bigram_vectorizer.transform(sstRec.X_test.values)\n",
    "X_test = bigram_tf_idf_transformer.transform(X_test)\n",
    "y_test = sstRec.y_test.values\n",
    "\n",
    "score = sgd_classifier.score(X_test, y_test)\n",
    "print(score)"
   ]
  }
 ]
}