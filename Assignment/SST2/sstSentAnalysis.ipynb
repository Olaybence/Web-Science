{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de",
   "display_name": "Python 3.8.5 64-bit (conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sklearn.datasets\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sort function\n",
    "take = lambda n, iterable: list(itertools.islice(iterable, n)) \n",
    "ascending = lambda array: dict(reversed(sorted(array.items(), key=lambda item: item[1])))\n",
    "\n",
    "class Analyzer:\n",
    "    def __init__(self, sourcePath):\n",
    "        self.data = pd.read_csv(sourcePath)\n",
    "        self.indexes = self.data['index']\n",
    "        self.labels = self.data['label']\n",
    "        self.texts = self.data['text']\n",
    "\n",
    "    def processData(self):\n",
    "        \n",
    "        # Create the slots for labels and grams\n",
    "        self.words = {}\n",
    "        # Label 0,1\n",
    "        for i in range(3):\n",
    "            # 1,2,3 - Grams\n",
    "            self.words[i] = {}\n",
    "            \n",
    "            for j in range(3):\n",
    "                self.words[i][j] = {}\n",
    "\n",
    "        # print(self.texts)\n",
    "        for i,text in enumerate(self.texts):\n",
    "            # if i > 5:\n",
    "            #     break\n",
    "            # print(i,text)\n",
    "            prev, prevPrev, current = \"\", \"\", \"\"\n",
    "            for j, word in enumerate(text.split(\" \")):\n",
    "                prevPrev = prev\n",
    "                prev = current\n",
    "                current = word\n",
    "                \n",
    "                # 1-Gram\n",
    "                if current not in self.words[self.labels[i]][0]:\n",
    "                    self.words[self.labels[i] ][0][current] = 1\n",
    "                else:\n",
    "                    self.words[self.labels[i] ][0][current] = self.words[self.labels[i] ][0].get(current) + 1\n",
    "\n",
    "                # Labels together\n",
    "                if current not in self.words[2][0]:\n",
    "                    self.words[2][0][current] = 1\n",
    "                else:\n",
    "                    self.words[2][0][current] = self.words[2][0].get(current) + 1\n",
    "\n",
    "                # 2-Gram\n",
    "                if j > 0:\n",
    "                    act2 = prev + ' ' + current\n",
    "                    # print('act2', act2)\n",
    "                    if act2 not in self.words[self.labels[i]][1]:\n",
    "                        self.words[self.labels[i] ][1][act2] = 1\n",
    "                    else:\n",
    "                        self.words[self.labels[i] ][1][act2] = self.words[self.labels[i] ][1].get(act2) + 1\n",
    "\n",
    "                    # Labels together\n",
    "                    if act2 not in self.words[2][1]:\n",
    "                        self.words[2][1][act2] = 1\n",
    "                    else:\n",
    "                        self.words[2][1][act2] = self.words[2][1].get(act2) + 1\n",
    "\n",
    "                # 3-Gram\n",
    "                if j > 1:\n",
    "                    act3 = prevPrev + ' ' + prev + ' ' + current\n",
    "                    # print('act3', act3)\n",
    "                    if act3 not in self.words[self.labels[i]][2]:\n",
    "                        self.words[self.labels[i] ][2][act3] = 1\n",
    "                    else:\n",
    "                        self.words[self.labels[i] ][2][act3] = self.words[self.labels[i] ][2].get(act3) + 1\n",
    "                    \n",
    "                    # Labels together\n",
    "                    if act3 not in self.words[2][2]:\n",
    "                        self.words[2][2][act3] = 1\n",
    "                    else:\n",
    "                        self.words[2][2][act3] = self.words[2][2].get(act3) + 1\n",
    "        print(\"process finished\")\n",
    "        # print(self.words[0][1])\n",
    "\n",
    "    def sortData(self):\n",
    "        self.sortedAscending = {}\n",
    "        # Label 0,1\n",
    "        for i in range(3):\n",
    "            # 1,2,3 - Grams\n",
    "\n",
    "            self.sortedAscending[i] = {}\n",
    "            for j in range(3):\n",
    "                # Sort all the words\n",
    "                self.sortedAscending[i][j] = ascending( self.words[i][j] )\n",
    "        print(\"data sorted\")\n",
    "\n",
    "    # Check the correctness by the first element's number\n",
    "    # in each subset (labeled with 1 and 0) and the whole.\n",
    "    def checkCounting(self):\n",
    "        # Check the first X item\n",
    "        checkedItems = 5\n",
    "        passed = True\n",
    "\n",
    "        for i in range(checkedItems):\n",
    "            if not passed:\n",
    "                break\n",
    "\n",
    "            # Grams\n",
    "            for j in range(3):\n",
    "                # Get the most frequent word of all in J - Gram\n",
    "                word = list(self.sortedAscending[2][j].items())[i][0]\n",
    "\n",
    "                # Counters\n",
    "                total = list(self.sortedAscending[2][j].items())[i][1]\n",
    "                label1 = self.sortedAscending[0][j].get(word)\n",
    "                label2 = self.sortedAscending[1][j].get(word)\n",
    "                if label1 + label2 != total:\n",
    "                    print('Checker failed: with the word ' + word)\n",
    "                    print(label1 + label2, '!=', total, ' difference:', abs( total - label1 - label2))\n",
    "                    print('The whole counter is not equal to the addition of label 0 and 1\\'s counter.')\n",
    "                    passed = False\n",
    "                    break\n",
    "                    \n",
    "        if passed:\n",
    "            print('Checking passed!')\n",
    "\n",
    "    def lengthAnalysis(self):\n",
    "        # Is there a collaration between the avarage length and label?\n",
    "        self.lengths = { 0: {}, 1: {} }\n",
    "        summ = [0,0]\n",
    "        count = [0,0]\n",
    "        for i, text in self.texts.items():\n",
    "            # Simple avarage length\n",
    "            la = self.labels[i]\n",
    "            summ[la] += len(text)\n",
    "            count[la] += 1\n",
    "            # Number of texts by length\n",
    "            if round(len(text),-1) in self.lengths[la]:\n",
    "                self.lengths[la][round(len(text),-1)] += 1\n",
    "            else:\n",
    "                self.lengths[la][round(len(text),-1)] = 1\n",
    "\n",
    "        plt.plot( self.lengths[1].keys(), self.lengths[1].values(), 'ro', markersize=2)\n",
    "        plt.plot( self.lengths[0].keys(), self.lengths[0].values(), 'bo', markersize=2)\n",
    "        plt.xlabel('length of reviews')\n",
    "        plt.ylabel('number of reviews')\n",
    "        print('Label 0 - Number of reviews:', count[0], '; The avarage length:', round(summ[0]/count[0], 2) )\n",
    "        print('Label 1 - Number of reviews:', count[1], '; The avarage length:', round(summ[1]/count[1], 2) )\n",
    "        print()\n",
    "        print('Label 1 - red; Label 0 - blue')\n",
    "        plt.show()\n",
    "\n",
    "    def lengthCorrelation(self):\n",
    "        lengths = [(lambda x: len(x[1]))(x) for x in self.texts.items()]\n",
    "        lenLab = pd.DataFrame( list(zip(lengths, self.labels)), columns=[\"length\",\"labels\"])\n",
    "        print(lenLab.corr())\n",
    "\n",
    "    def printFrequent(self,n):\n",
    "        # The first 5 most frequent elements\n",
    "        for j in range(3):\n",
    "            for i in range(3):\n",
    "                if i == 2:\n",
    "                    # Together the labels\n",
    "                    print('Counting the whole G-' + str(j+1))\n",
    "                else:\n",
    "                    # i - Labels, j - Grams\n",
    "                    print('L-' + str(i) + ' G-' + str(j+1) + ' (in ascending order): ')\n",
    "\n",
    "                words = ''\n",
    "                scores = ''\n",
    "                for k,word in enumerate(take(n, train.sortedAscending[i][j].items())):\n",
    "                    if k == 0:\n",
    "                        words += '\\'' + word[0] + '\\''\n",
    "                        scores += str(word[1])\n",
    "                    else: \n",
    "                        words += ', \\'' + word[0] + '\\''\n",
    "                        scores += ', ' + str(word[1])\n",
    "                print('words:', words)\n",
    "                print('scores:', scores)\n",
    "                print()\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Analyzer(\"stsa.binary.phrases.train\")\n",
    "train.processData()\n",
    "train.sortData()\n",
    "train.checkCounting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.lengthAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.lengthCorrelation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.printFrequent(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "## DEV ##\n",
    "#########\n",
    "dev = Analyzer(\"stsa.binary.dev\")\n",
    "dev.processData()\n",
    "dev.sortData()\n",
    "dev.checkCounting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.lengthAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.lengthCorrelation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.printFrequent(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "## TEST ##\n",
    "##########\n",
    "test = Analyzer(\"stsa.binary.test\")\n",
    "test.processData()\n",
    "test.sortData()\n",
    "test.checkCounting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.lengthCorrelation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.lengthAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.printFrequent(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 9.76616042  7.54153687  8.61348091 ... 10.86477271 10.4593076\n 10.4593076 ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class ExtendedSentAnal(Analyzer):\n",
    "    cv = CountVectorizer()\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "\n",
    "    def __init__(self, sourcePath):\n",
    "        Analyzer.__init__(self, sourcePath)\n",
    "    \n",
    "    def tfIdfWithCV(self):\n",
    "        word_count_vector = cv.fit_transform(self.texts)\n",
    "        print(word_count_vector)\n",
    "        # sort ascending \n",
    "        # df_idf = pd.DataFrame(word_count_vector.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n",
    "    \n",
    "    def tfIdfWithTranformer(self):\n",
    "        tfidf_transformer.fit(word_count_vector)\n",
    "        # print(word_count_vector)\n",
    "        df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n",
    "        # sort ascending \n",
    "        print(tfidf_transformer.idf_)\n",
    "        # print(df_idf.sort_values(by=['idf_weights']))\n",
    "\n",
    "\n",
    "train = ExtendedSentAnal(\"stsa.binary.phrases.train\")\n",
    "#instantiate CountVectorizer() \n",
    "train.tfIdfWithTranformer()\n",
    " \n",
    "# this steps generates word counts for the words in your docs \n",
    "\n",
    "\n",
    "\n",
    "# print idf values \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count matrix \n",
    "count_vector=cv.transform(train.texts) \n",
    " \n",
    "# tf-idf scores \n",
    "tf_idf_vector = tfidf_transformer.transform(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               tfidf\n",
       "dredge      0.433279\n",
       "nerds       0.400452\n",
       "revenge     0.366934\n",
       "clichés     0.332826\n",
       "filmmakers  0.313861\n",
       "...              ...\n",
       "fidgeted    0.000000\n",
       "field       0.000000\n",
       "fields      0.000000\n",
       "fiend       0.000000\n",
       "élan        0.000000\n",
       "\n",
       "[15093 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tfidf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>dredge</th>\n      <td>0.433279</td>\n    </tr>\n    <tr>\n      <th>nerds</th>\n      <td>0.400452</td>\n    </tr>\n    <tr>\n      <th>revenge</th>\n      <td>0.366934</td>\n    </tr>\n    <tr>\n      <th>clichés</th>\n      <td>0.332826</td>\n    </tr>\n    <tr>\n      <th>filmmakers</th>\n      <td>0.313861</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>fidgeted</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>field</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>fields</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>fiend</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>élan</th>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>15093 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "feature_names = cv.get_feature_names() \n",
    " \n",
    "#get tfidf vector for first document \n",
    "first_document_vector=tf_idf_vector[0]\n",
    " \n",
    "#print the scores \n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}